{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Graded Case 1 (Part I): House price prediction\n",
        "\n",
        "In this case (Part I), you will build a multilayer perceptron network to predict the selling price of properties. The dataset consists of all single family houses and condos that were sold in Denver in a given year.\n",
        "\n",
        "You need to submit the following files:\n",
        "\n",
        "- A pdf or word document containing the plots of the training errors for the multi-layer perception model and the linear regression model, and the answers to the two questions below.\n",
        "\n",
        "- The complete Juyputer notebook containing all your Pytorch code with explanations, along with a Markdown text explaining different parts if needed.\n",
        "\n",
        "To get the test error for your model, you need to submit your predicted prices for test data on Kaggle. Note that in Part I of the case, you do not need to worry about optimizing your model to get the lowest error possible. The Part I will be graded based on your implemention of the base model as specified below.  We will come back to optimize the model and the Kaggle competition in Part II of the case."
      ],
      "metadata": {
        "id": "eqM8VQAFpTst"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s4QjqzSng9P7"
      },
      "source": [
        "---\n",
        "## Data Loading and Visualize Data\n",
        "\n",
        "The train data and test data are available on Kaggle website.\n",
        "You can first download them, then upload them to the google colab, and then read the data using pandas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K4gWVq036dy1",
        "scrolled": true,
        "ExecuteTime": {
          "end_time": "2023-08-14T16:46:12.975303400Z",
          "start_time": "2023-08-14T16:46:12.877462900Z"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd  # Importing pandas, which is a library for data manipulation and analysis\n",
        "#TODO: Read the datasets\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a quick look at the data."
      ],
      "metadata": {
        "id": "5PYoHVNRs0fJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the train dataframe\n",
        "print(train_df.shape)\n",
        "print(train_df.columns)"
      ],
      "metadata": {
        "id": "CNI8l_IqszRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the test dataframe\n",
        "print(test_df.shape)\n",
        "print(test_df.columns)"
      ],
      "metadata": {
        "id": "dVx9XOWLs9l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, we have 11581 training samples and 4964 test samples, each with 16 features. The training samples contain the sale_prices, which are the labels. The test samples do not contain the sale_prices, which we will predict by building a MLP model."
      ],
      "metadata": {
        "id": "FrAjbstvtKiz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of SALE PRICES in train data\n",
        "\n",
        "Let's take a closer look at the sale prices in the train data."
      ],
      "metadata": {
        "id": "NFUBVOm_1bZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt  # Importing matplotlib's pyplot, it provides a MATLAB-like interface for making plots and charts\n",
        "\n",
        "# Set the style\n",
        "sns.set(style=\"whitegrid\")\n",
        "\n",
        "# Create a histogram\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.histplot(train_df['SALE_PRICE'], bins=50, color='blue')\n",
        "plt.title('Histogram of Sale Prices (Train Data)')\n",
        "plt.xlabel('Sale Price')\n",
        "plt.ylabel('Number of Properties')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ndstEecX0cOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_df['SALE_PRICE'].min())\n",
        "print(train_df['SALE_PRICE'].max())\n",
        "print(train_df['SALE_PRICE'].median())"
      ],
      "metadata": {
        "id": "RbVfvOcC1pO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the sale_price has a wide range from 50K to 2 million, with the median price 431K."
      ],
      "metadata": {
        "id": "iEUy_v411r50"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Data Preparation\n",
        "\n",
        "The first step when building a neural network model is getting your data into the proper form to feed into the network.\n",
        "\n",
        "- **Train labels**: We need to extract the sale prices from the train data as train labels. Since the house prices can take very large values, to make training fast it is helpful to define the train labels as the sale prices divided by a normalization factor.\n",
        "\n",
        "- **Handing non-numeric features**: Some of the house features are non-numeric. We will learn about how to process categorical data in the upcoming lectures. For now, you can  remove those non-numeric features and only train over the numeric features.\n",
        "\n",
        "- **Feature standardization**: When predicting house prices, you started from features that took a variety of ranges—some features had small floating-point values, and others had fairly large integer values. The model might be able to automatically adapt to such heterogeneous data, but it would definitely make learning more difficult. A widespread best practice for dealing with such data is to do feature-wise normalization: for each feature in the input data (a column in the input dataframe), we subtract the mean of the feature and divide by the standard deviation, so that the feature is centered around 0 and has\n",
        "a unit standard deviation. Note that here we combine the feature vectors in the train and test data. In this way, the train and test data go through the same normalization.\n",
        "\n",
        "- **Handling missing values**: There may exist some entries with missing values. After the feature standardization, we can impute the missing values with zeros."
      ],
      "metadata": {
        "id": "PHY3Ye4wCmWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the sale_price in train data has a wide range from 50K to 2 million, with the median price 431K. We can divide the sale_price by 100K, so the normalized sale_price is between 0.5 and 20 in training data. Remember, when we output the predicted price for the test data, we need to multiply back the normalization factor."
      ],
      "metadata": {
        "id": "Qxl_Ns_y1FDf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: define labels for train data\n",
        "normalization_factor=100000\n",
        "train_labels = train_df['SALE_PRICE']/normalization_factor\n",
        "train_df.drop('SALE_PRICE', axis=1, inplace=True) # drop the sale_prices in features."
      ],
      "metadata": {
        "id": "Zp02cVWcBOJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inplace parameter, when set to True , allows you to drop the rows or columns without returning a new DataFrame. The issue arises when the drop function reorders the DataFrame, which can be problematic when the order of your data matters"
      ],
      "metadata": {
        "id": "LtT2LBC6exNU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels.shape"
      ],
      "metadata": {
        "id": "QjV866fJBuNt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels"
      ],
      "metadata": {
        "id": "kGKG5wrO2BTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that both the training samples and test samples contain an ID column, which is not informative for predicting the house price. Thus we will drop the ID column."
      ],
      "metadata": {
        "id": "sLxt5zN1t-xi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ID=train_df['ID']\n",
        "test_ID=test_df['ID']\n",
        "train_df.drop('ID', axis=1, inplace=True)\n",
        "test_df.drop('ID', axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "gxE-GQSCtTQ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Then we combine the feature vectors in the train data and test data\n",
        "features=pd.concat(objs=[train_df,test_df],axis=0)"
      ],
      "metadata": {
        "id": "iD1nAkNuuXR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numeric_features = features.dtypes[features.dtypes != 'object'].index\n",
        "non_numeric_features = features.dtypes[features.dtypes == 'object'].index\n",
        "numeric_features, non_numeric_features"
      ],
      "metadata": {
        "id": "G9b-YwAsucEC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there are three non-numeric features, namely `NBHD`, `PROP_CLASS`, and `STYLE_CN`. We will apply one-hot encoding to those non-numeric features in our model; you could also simply drop these non-numeric features."
      ],
      "metadata": {
        "id": "-ClwTmu7unSA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features= features.drop(non_numeric_features, axis=1)"
      ],
      "metadata": {
        "id": "hKSrMymXuhLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Standardize numeric features\n",
        "features[numeric_features] = features[numeric_features].apply(lambda x: (x - x.mean()) / (x.std()))"
      ],
      "metadata": {
        "id": "B7on9kbjuuiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# recheck the mean and std after standardization\n",
        "features[numeric_features].mean(), features[numeric_features].std()"
      ],
      "metadata": {
        "id": "z4krtIsTuyfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that after standardization, the features for the train data have mean 0 and standard deviation 1."
      ],
      "metadata": {
        "id": "bvaIEWmhu21k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# After the feature standardization, we can impute the missing values with zeros.\n",
        "features[numeric_features] = features[numeric_features].fillna(0)"
      ],
      "metadata": {
        "id": "3h4klXSOu5qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Double check the features after data processing."
      ],
      "metadata": {
        "id": "xd1kFir0u_6t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features.info()\n",
        "print(features.columns)"
      ],
      "metadata": {
        "id": "6Cnl5vwQvBp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we are left with 12 features."
      ],
      "metadata": {
        "id": "mNhhtspzvY5D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# check whether there is any missing entry\n",
        "print(features.isnull().sum())"
      ],
      "metadata": {
        "id": "9AISLklJvELM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Write code to construct feature vectors for train and test data after data preparation.\n",
        "train_features = features.iloc[:len(train_labels)]\n",
        "test_features = features.iloc[len(train_labels):]"
      ],
      "metadata": {
        "id": "ROUIrsA0Bn1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_features.shape, test_features.shape"
      ],
      "metadata": {
        "id": "lJA07PMIBjPj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we convert features and labels to PyTorch tensors."
      ],
      "metadata": {
        "id": "7nclPlDEBtMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# Convert training features and labels to PyTorch tensors\n",
        "train_features = torch.tensor(train_features.values.astype(np.float32), dtype=torch.float32)\n",
        "test_features = torch.tensor(test_features.values.astype(np.float32), dtype=torch.float32)\n",
        "train_labels = torch.tensor(train_labels.values.reshape(-1, 1).astype(np.float32), dtype=torch.float32)"
      ],
      "metadata": {
        "id": "XcjojyA1r6g1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## DataLoaders and Batching\n",
        "\n",
        "After creating training, test, and validation data, we can create DataLoaders for this data by following two steps:\n",
        "1. Create a known format for accessing our data, using [TensorDataset](https://pytorch.org/docs/stable/data.html#torch.utils.data.TensorDataset) which takes in an input set of data and a target set of data with the same first dimension, and creates a dataset.\n",
        "2. Create DataLoaders and batch our training, validation, and test Tensor datasets. Note that we will shuffle the train data, so the model will not learn a particular order. For test data, we do not shuffle."
      ],
      "metadata": {
        "id": "GpfNC1455HYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "#  Create DataLoaders and batch our train data\n",
        "train_data = TensorDataset(train_features, train_labels)\n",
        "train_loader = DataLoader(train_data, batch_size=128, shuffle=True)"
      ],
      "metadata": {
        "id": "iFnex-AG5hu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Create DataLoaders and batch our test data\n",
        "test_data =TensorDataset(test_features)\n",
        "test_loader = DataLoader(test_data, batch_size=128, shuffle=False)"
      ],
      "metadata": {
        "id": "sL-N-Sr5AGey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a batch to have a sanity check"
      ],
      "metadata": {
        "id": "IdAZUijCBb2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# obtain one batch of training data\n",
        "dataiter = iter(train_loader)\n",
        "features, labels = next(dataiter)\n",
        "\n",
        "print('Sample input size: ', features.size()) # batch_size, seq_length\n",
        "print('Sample input: \\n', features)\n",
        "print()\n",
        "print('Sample label size: ', labels.size()) # batch_size\n",
        "print('Sample label: \\n', labels)"
      ],
      "metadata": {
        "id": "qEyW4FHfC-gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Linear Regression as Benchmark\n",
        "\n",
        "Let us build a linear regression model as a benchmark. Note that the linear regression model can be viewed as a special instance of multi-layer perception with no hidden layer and a single output neuron."
      ],
      "metadata": {
        "id": "BQ7a1MTxbK_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Build a linear regression model network\n",
        "import torch.nn as nn\n",
        "lin_net = nn.Linear(train_features.shape[1], 1)"
      ],
      "metadata": {
        "id": "c-OjHSVBEP90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's print out the model achitecture."
      ],
      "metadata": {
        "id": "isN2VEtI_S_3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lin_net"
      ],
      "metadata": {
        "id": "3QCDGgFq_SOH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a batch and see the output\n"
      ],
      "metadata": {
        "id": "J8qrOALB_HkH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features, labels = next(dataiter)\n",
        "output=lin_net(features)\n",
        "output.shape,labels.shape"
      ],
      "metadata": {
        "id": "SnhqS64E_Fcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "4gP6KXY_TAR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will use GPU training if it is availabe."
      ],
      "metadata": {
        "id": "2EBhmGZh4Gt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: use GPU for training if it is availabe\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "lin_net = lin_net.to(device)"
      ],
      "metadata": {
        "id": "CFheqQ4Q4L49"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second, let us specify the loss function."
      ],
      "metadata": {
        "id": "SKEzpB5F5FCa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: specify the loss function for training\n",
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "_rEH9aVe4_7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now ready to train the network."
      ],
      "metadata": {
        "id": "TweT3hAxadKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that with house prices, as with stock prices, we care about relative quantities more than absolute quantities. Thus we tend to care more about the relative error than about the absolute error. For instance, if our prediction is off by \\\\$100,000 when estimating the sale price of a house which is \\\\$125,000, then we are probably doing a horrible job. On the other hand, if we err by this amount for a house with sale price \\\\$2 million, this might represent a pretty  accurate prediction.\n",
        "\n",
        "To this end, we will use the median error rate (MER) used by [Zestimate](https://www.zillow.com/z/zestimate/) to measure the predictive performance. The error rate is defined as\n",
        "$$\n",
        "\\text{Error Rate} = \\left| \\frac{\\text{Predicted Price}-\\text{Actual Price}}{\\text{Actual Price}} \\right|\n",
        "$$\n",
        "The median error rate is defined as the median of error rates for all properties."
      ],
      "metadata": {
        "id": "fRQazFvtxbDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Write code to train the network\n",
        "def train(model, train_loader, num_epochs, learning_rate=0.001):\n",
        "    train_losses = []                                       # To store training losses\n",
        "    train_errors = []                                       # To store training error rates\n",
        "    # Initialize the optimizer, we use Adam here\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    for epoch in range(num_epochs):\n",
        "        tot_train_loss = 0.0                            # Store the total training MSE loss for this epoch\n",
        "        train_error_rates =torch.tensor([]).to(device)  # Store the training error rates for this epoch\n",
        "        for features, labels in train_loader:           # For each batch in DataLoader\n",
        "            # Move data to device (GPU or CPU)\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            output = model(features)  # Compute the output\n",
        "            loss = criterion(output, labels)  # Compute loss\n",
        "            optimizer.zero_grad()  # Clear the gradients from previous iteration\n",
        "            loss.backward()  # Backpropagate the error\n",
        "            optimizer.step()  # Update the weights\n",
        "\n",
        "            tot_train_loss += loss.item()\n",
        "\n",
        "            # Compute and store error rate for training data\n",
        "            train_error_rate = torch.absolute(output/labels-1)\n",
        "            train_error_rates= torch.cat((train_error_rates,train_error_rate),dim=0)\n",
        "\n",
        "\n",
        "        train_loss = tot_train_loss /len(train_loader)\n",
        "        train_error = torch.median(train_error_rates).item()\n",
        "        train_losses.append(train_loss)\n",
        "        train_errors.append(train_error)\n",
        "        # Print the training loss and validation loss every 10 epochs\n",
        "        if epoch % 10 == 0:\n",
        "            print(\"Epoch: {}/{}.. \".format(epoch+1, num_epochs),\n",
        "              \"Train Loss: {:.3f}.. \".format(train_loss),\n",
        "              \"Train Median Error Rate: {:.3f}.. \".format(train_error),\n",
        "              )\n",
        "\n",
        "    return  train_losses,train_errors"
      ],
      "metadata": {
        "id": "kUGatRu_DxzT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses, train_errors = train(lin_net, train_loader, num_epochs=500, learning_rate=0.001)"
      ],
      "metadata": {
        "id": "PFjx_B8S-3oz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the training error (MER) over epochs"
      ],
      "metadata": {
        "id": "TpNjElY1D3xa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Write code to plot the training error (MER) over epochs\n",
        "import matplotlib.pyplot as plt\n",
        "def plot_errors(train_errors, num_epochs):\n",
        "    plt.figure(figsize=(10, 7))\n",
        "    plt.plot(range(1, num_epochs + 1), train_errors, label='train')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('median error rate')\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "k9U1YyMmEV9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_errors(train_errors, num_epochs=500)"
      ],
      "metadata": {
        "id": "N-2iSGCtxBi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Build the Multi-layer Perceptron Base Model"
      ],
      "metadata": {
        "id": "vsxR1nrM1yFH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the following, we build a multi-layer perception model."
      ],
      "metadata": {
        "id": "2hZc2xpgZxmj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Build a multi-layer perception neural network with 2 hidden layers of sizes 256 and 128, respectively and ReLu activations\n",
        "import torch.nn as nn\n",
        "model = nn.Sequential(nn.Linear(train_features.shape[1], 256),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(256, 128),\n",
        "                      nn.ReLU(),\n",
        "                      nn.Linear(128,1))"
      ],
      "metadata": {
        "id": "8tYjrMNm2FXP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=model.to(device)"
      ],
      "metadata": {
        "id": "d3nnr_hR-80s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: write code to train the MLP network\n",
        "train_losses, train_errors = train(model, train_loader, num_epochs=500, learning_rate=0.001)"
      ],
      "metadata": {
        "id": "HqW8Z5FC_kQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Write code to plot the training error (MER) over epochs\n",
        "plot_errors(train_errors, num_epochs=500)"
      ],
      "metadata": {
        "id": "_ICAPr4-_lab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1**: What are your final training errors of the multilayer perception model and the linear regression model?\n",
        "\n",
        "We can see that the training error of the linear regression model is around 22% and training error of the multilayer perception model is around 10%."
      ],
      "metadata": {
        "id": "GeJy5irS_zwo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Inference on test data\n",
        "\n",
        "After the MLP model is trained, we can use it for inference."
      ],
      "metadata": {
        "id": "08gC_-Q4pDon"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " We need to remember to set the model in inference mode with model.eval(). You'll also want to turn off autograd with the torch.no_grad() context."
      ],
      "metadata": {
        "id": "ZNOrrfiC7dlb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: write the code to generate predicted sale prices for test data\n",
        "# Test our network for one batch\n",
        "model.eval()\n",
        "dataiter = iter(test_loader)\n",
        "features,= next(dataiter)\n",
        "features=features.to(device)\n",
        "output = model(features)"
      ],
      "metadata": {
        "id": "jP5GJ4tCpHG8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    # In evaluation (or testing) mode, we don't want any parameter updates. We want the model to give\n",
        "    # the final output based on current parameter values.\n",
        "    pred_labels = torch.tensor([]).to(device)\n",
        "    with torch.no_grad():\n",
        "      for features, in test_loader:\n",
        "            # Move data to device (GPU or CPU)\n",
        "            features = features.to(device)\n",
        "            output = model(features)  # Compute the output\n",
        "            # store prediction results\n",
        "            pred_labels = torch.cat( (pred_labels,output),dim=0)\n",
        "    return  pred_labels"
      ],
      "metadata": {
        "id": "kNoxQnSe7jZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels =test(model,test_loader)"
      ],
      "metadata": {
        "id": "1XT0_J1r7lJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember when predict the price, we need to multiply back the normalization factor"
      ],
      "metadata": {
        "id": "37dUmKdF7oNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels=pred_labels*normalization_factor"
      ],
      "metadata": {
        "id": "8IB8O02q7qQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_labels[:20]"
      ],
      "metadata": {
        "id": "1LNTnQvG7r59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = {'ID': test_ID, 'SALE_PRICE': pred_labels.cpu().numpy().squeeze()}\n",
        "pred_df=pd.DataFrame(data)\n",
        "pred_df.head()"
      ],
      "metadata": {
        "id": "dwp-2GuW7v6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the dataframe to CSV file without index by setting index=False."
      ],
      "metadata": {
        "id": "ybK6TRB5pjBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: save the predicted sale prices into submission_csv\n",
        "pred_df.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "LluvvUz97xlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can submit our predictions on Kaggle and see how they compare with the actual house prices (labels) on the test set.\n",
        "\n",
        "- Log in to the Kaggle website and visit the house price prediction competition page.\n",
        "\n",
        "- Click the “Submit Predictions” button.\n",
        "\n",
        "- Click the “Upload Submission File” button in the dashed box at the bottom of the page and select the prediction file you wish to upload.\n",
        "\n",
        "- Click the “Make Submission” button at the bottom of the page to view your results."
      ],
      "metadata": {
        "id": "gNje9TjRphTx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 2**: What is the test error shown on Kaggle? How does it compare with the train error?"
      ],
      "metadata": {
        "id": "28-wqckhrA0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It turns out that the test error is around 15%, which is much higher than the train error."
      ],
      "metadata": {
        "id": "TvtvjuDD-VD6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Evaluate test error\n",
        "\n",
        "**Note**: To evaluate test error, you need to have access to the true sale prices for test error. No need to run the following code!!! Instead, you will get the test error by submitting your predictions on Kaggle as described above!"
      ],
      "metadata": {
        "id": "Z7VFhrS_8KRZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_sol=pd.read_csv('solution.csv')\n",
        "df_sol.head()\n"
      ],
      "metadata": {
        "id": "bRWp9Jnc9B2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels=df_sol['SALE_PRICE'].values"
      ],
      "metadata": {
        "id": "QSCc-1xc9IYy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_labels.shape"
      ],
      "metadata": {
        "id": "BzWkbiOz9dvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute and store median error rate for validation data\n",
        "test_errors=pred_labels.cpu().numpy().squeeze()/test_labels-1\n",
        "test_errors.shape"
      ],
      "metadata": {
        "id": "cnYxYkes8N-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "median_test_error=np.median(np.absolute(test_errors)).item()"
      ],
      "metadata": {
        "id": "CKAo0PIa9lvC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "median_test_error"
      ],
      "metadata": {
        "id": "6rWHO55E9qnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusion\n",
        "\n",
        "We see that the median test error is around 15%. In Part II of Case 1, you will be asked to vary model architecture or optimization algorithms to see if you can squeeze out a lower test error."
      ],
      "metadata": {
        "id": "dC1WW3Mo9sXf"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "s4QjqzSng9P7",
        "PHY3Ye4wCmWM",
        "GpfNC1455HYf",
        "BQ7a1MTxbK_T",
        "4gP6KXY_TAR3",
        "vsxR1nrM1yFH",
        "08gC_-Q4pDon",
        "Z7VFhrS_8KRZ"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}